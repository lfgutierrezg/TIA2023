Person tracking and counting systems require detection of persons in images. The practical implementations of the systems incline towards processing of orthogonally captured images on edge computing devices. The use of bounding box-based detectors in such implementations of the systems is rather counterproductive due to an indirect proportion between detection performance and inference time. The ellipse-like shape of heads in orthogonally captured images inspired us to predict head centroids instead of bounding boxes. We predict the centroids using a fully convolutional network (FCN). We combine the FCN with simple image processing operations to ensure fast inference of the detector. We experiment with the size of the FCN output to further decrease the inference time. We compare the proposed centroid-based detector with bounding box-based detectors on the head detection task} in terms of the inference time and the detection performance. We propose a performance measure which allows quantitative comparison of the two detection approaches. For the training and evaluation of the detectors, we form original datasets of 8000 annotated images in total, which are characterized by high variability in terms of lighting conditions, background, image quality, and elevation profile of scenes. To simplify creation of the datasets, we propose an approach which allows simultaneous annotation of the images for both bounding box-based and centroid-based detection. The centroid-based detector shows, on the datasets, best detection performance while keeping edge computing standards.

Dataset
Quality of datasets predetermines performance of deep ConvNet-based computer vision systems. To ensure robustness of the person detectors in the intended setting, we collect data in diverse environments which include staircases, corridors, and entries into means of transport. We capture video streams with the RealSense camera D435 orthogonally placed above walking persons at eight different locations. The walking persons are adults with and without headgear. The head-lens distance varies between 25 and 100 cm depending on the environment and situation in the scene. This setting results in large variance in the size of the heads and their sharpness. The lighting conditions differ among the experiments.

We extract frames from the captured videos to create a set of 8-bit RGB images. From the frames of seven locations, we cut out 7000 square images with up to nine persons. We randomly split the set of images in the ratio 6:1 to create training and evaluation datasets (Training_set_Cropped288 and Testing_set_Cropped288). From the eighth location, we form a blind evaluation dataset (Testing_set_Blind_Cropped288) of 1000 square image crops. As these images are captured under different lighting conditions at a different place from the previous locations, Testing_set_Blind_Cropped288 allows testing the generalization capabilities of the detectors. We resize images in all datasets to 288 x 288 px.

For all datasets, the target images are prepared the respective folders.